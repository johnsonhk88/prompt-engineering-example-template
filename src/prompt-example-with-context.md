
## Prompt 1: Summarize YouTube transcript
Prompt: Summarize the following in 5 bullet points.
Title: "(5362) Normalizing Inputs (C2W1L09) - YouTube"
Transcript: "when training a neural network one of the techniques to speed up your training is if you normalize your inputs let's see what that means let's see the training sets with two input features so the input features X are two-dimensional and here's a scatterplot of your training set normalizing your inputs corresponds to two steps the first is to subtract out or to zero out the mean so you set mu equals 1 over m sum over I of X I so this is a vector and then X gets set as X minus mu for every training example so this means you just move the training set until it has zero mean and then the second step is to normalize the variances so notice here that the feature x1 has a much larger variance than the feature x2 here so what we do is set Sigma equals 1 over m sum of X I star saw two I guess this is element wise squaring and so now Sigma squared is a vector with the variances of each of the features and notice we've already subtracted out the means so X I squared element Y squared is just the variances and you take you an example and divide it by you know this vector Sigma squared and so in pictures you end up with this where now the variance of x1 and x2 are both equal to 1 oh into one tip if you use this to scale your training data then use the same mu and Sigma squared to normalize your test set right in particular you don't want to normalize the training set and the test set differently whatever this value is and whatever this value is use them in you know these two formulas so that you scare your test set in exactly the same way rather than estimating mu and Sigma squared separately on your training set and test because you want your data both training and test examples to go through the same transformation defined by the same Mew and Sigma squared calculated on your training data so why do we do this why do we want to normalize the input features recall that the cost function is defined as written on top right it turns out that if you use unnormalized input features is more likely that your cost function will look like this at a very squished out bow very elongated cost function where you know the minimum you're trying to find this maybe over there but if you're beakers are on very different scales say the feature x1 ranges from 1 to 1000 and the feature x2 ranges from 0 to 1 then it turns out that ratio or the range of values for the parameters w1 and w2 will end up taking on very different values and so maybe these axes should be w1 and w2 probe intuition I'll plot W and B be a cost function can be a very elongated bow like that so if you plot the contours of this function you can have a very elongated function like that whereas if you normalize the features then your cost function well on average look more symmetric and if you are running bathe in the scent on the cost function like the one on the left then you might have to use a very small learning rate because over here you know the grading descent might need a lot of steps to oscillate back and forth right before it finally finds its way to the minimum whereas if you have a more spherical contours than wherever you start breathing descents can pretty much go straight to the minimum you can take much larger steps but gradient descent need rather than needing to oscillate around like the picture on the left of course in practice W is a high dimensional vector and so trying to plot this in 2d doesn't convey all the intuitions correctly but the rough intuition that your cost function will be you know more round and easier to optimize your features are all on similar skills not all not from 1 to 1000 0 to 1 but mostly from your minus 1 to 1 or with about similar variants as each other that just makes your cost function J easier and faster to optimize in practice if one feature say x1 ranges from 0 to 1 and x2 ranges from minus 1 to 1 and x3 ranges from 1 to 2 you know these are fairly similar ranges so this will work just fine is when there are dramatically different ranges like ones from one to a thousand and another from zero to one that that really hurts the optimization algorithm but by just setting all of them to zero mean and say variance 1 like we did in the last slide that just guarantees that all your features are similar scale and will usually help your learning Avrum run faster so if your input features came from very different scales maybe some features are from 0 to 1 some from 1 to 1000 then it's important to normalize your features if your features came in on similar skills in this step is less important although performing this type of normalization pretty much never does any harm so often you know do it anyway if I'm not sure whether or not they were help with speeding up training for your algorithm so that's it for normalizing your input features next let's keep talking about ways to speed up the training of your new network"



### Prompt 2 
prompt : you are act as data scientist for financial markets. you have a lot of experence to predict USA stock market price. your task building AI model predicting the closing price movements for hundreds of Nasdaq listed stocks using data from the order book and the closing auction of the stock. Information from the auction can be used to adjust prices, assess supply and demand dynamics, and identify trading opportunities. You have the train dataset with the follow {columnlist}, How to feature selected and feature engineering to enhanace the AI model performance. Can you explain each feature how to efficiently predict to stock market price.

columnlist = f"""
- stock_id - A unique identifier for the stock. Not all stock IDs exist in every time bucket.
- date_id - A unique identifier for the date. Date IDs are sequential & consistent across all stocks.
- imbalance_size - The amount unmatched at the current reference price (in USD).
- imbalance_buy_sell_flag - An indicator reflecting the direction of auction imbalance.
- buy-side imbalance; 1
- sell-side imbalance; -1
- no imbalance; 0
- reference_price - The price at which paired shares are maximized, the imbalance is minimized and the - distance from the bid-ask midpoint is minimized, in that order. Can also be thought of as being equal to the near price bounded between the best bid and ask price.
- matched_size - The amount that can be matched at the current reference price (in USD).
- far_price - The crossing price that will maximize the number of shares matched based on auction interest - only. This calculation excludes continuous market orders.
- near_price - The crossing price that will maximize the number of shares matched based auction and continuous - market orders.
- [bid/ask]_price - Price of the most competitive buy/sell level in the non-auction book.
- [bid/ask]_size - The dollar notional amount on the most competitive buy/sell level in the non-auction book.
- wap - The weighted average price in the non-auction book.

- seconds_in_bucket - The number of seconds elapsed since the beginning of the day's closing auction, always starting from 0.
- target - The 60 second future move in the wap of the stock, less the 60 second future move of the synthetic -  index. Only provided for the train set.
"""